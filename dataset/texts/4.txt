BEST POLYNOMIAL APPROXIMATION FOR NON-AUTONOMOUS
LINEAR ODES IN THE ⋆-PRODUCT FRAMEWORK∗
STEFANO POZZA†
Abstract. We present the first formulation of the optimal polynomial approximation of the
solution of linear non-autonomous systems of ODEs in the framework of the so-called ⋆-product.
This product is the basis of new approaches for the solution of such ODEs, both in the analytical
and the numerical sense. The paper shows how to formally state the problem and derives upper
bounds for its error.
Key words. Non-autonomous linear ODEs, polynomial approximation, error analysis,
AMS subject classifications. 46F10, 37C60, 65L05
1. Introduction. Recently, a new approach has been introduced for the solution
of systems of linear non-autonomous ordinary differential equations based on the socalled ⋆-product [6, 7, 12, 16]. Such an approach has proved to be valuable and
effective analytically, by producing new explicit expressions for the solution of certain
problems [5, 6, 8], and numerically, being the basis of new efficient algorithms in
quantum chemistry problems [3, 13, 14, 15].
Given a Hermitian matrix-valued function A˜(t) ∈ C
N×N analytic on the bounded
interval I, and the nonzero vector ˜v ∈ C
N×N we consider the initial value problem
∂
∂tu˜(t) = A˜(t)˜u(t), u˜(a) = ˜v, t ∈ I = [a, b]. (1.1)
The ⋆-product is defined over a Fr´echet-Lie group on distributions [16]. In such a
group, the initial value problem becomes a ⋆-linear system. Thanks to this “linearization” of the ODE, new techniques can be applied to solve the problem. Here, we
focus on the polynomial approximation approach, which can be used both in numerical approaches and in the theoretical framework. In particular, in the latter one,
a symbolic algorithm named ⋆-Lanczos [8] is able to produce a Krylov-like subspace
approximation, that is, a polynomial approximation in the ⋆-product sense.
In this work, we will show that it is possible to formulate the problem of a best
polynomial approximation for ˜u in the ⋆-framework. Moreover, we will show that its
error can be bounded by the best uniform norm polynomial approximation error for
the exponential [11]. This result is crucial to understand the numerical behavior of
polynomial-based numerical methods when solving linear systems derived by using
the ⋆-approach. Indeed, the polynomial approximation is central in the analysis of
standard Krylov subspace methods (e.g., [10]), and its extension to the ⋆-framework
will also allow extending this kind of numerical analysis.
In Section 2, we introduce the basics of the ⋆-product, and we state the main
result. Section 3 shows how to extend matrix analysis results to the ⋆-framework. The
prove of the main result is given in Section 4 and Section 5 draw some conclusions.
∗This work was supported by Charles University Research Centre program No.
PRIMUS/21/SCI/009 and UNCE/24/SCI/005, and by the Magica project ANR-20-CE29-0007
funded by the French National Research Agency.
†Faculty of Mathematics and Physics, Charles University, Sokolovsk´a 83, 186 75 Praha 8, Czech
Republic,(pozza@karlin.mff.cun.cz).
1
2 S. Pozza
Table 1.1
List of the main ⋆-definitions and related objects, f, g, x are generally from A(I).
Name Symbol Definition Comments / Properties
⋆-product f ⋆ g R
I
f(t, τ)g(τ, s)dτ
⋆-identity δ f ⋆ δ = δ ⋆ f = f δ(t − s): Dirac delta
⋆-inverse f−⋆ f ⋆ f−⋆ = f−⋆ ⋆ f = δ Existence [7, 16]
Heaviside function Θ Θ(t − s) = 1, t ≥ s, 0 otherwise
Analytic Θ-set AΘ(I) {f˜(t, s)Θ(t − s): f˜ analytic on I
2} ⋆-product-closed set
Dirac 1st derivative δ
′
δ
′
(t − s) δ
′ ⋆ Θ = Θ ⋆ δ′ = δ
Dirac derivatives δ
(j) δ
(j)
(t − s) δ
(j) ⋆ δ(i) = δ
(i+j)
⋆-powers f
⋆j f ⋆ f ⋆ · · · ⋆ f, j times f
⋆0
:= δ, by convention
⋆-resolvent R⋆(x)
P∞
j=0 x
⋆j , x ∈ AΘ(I) R⋆(x) = (δ − x)−⋆
⋆-polynomial p
⋆(x)
Pn
j=0 αjx
⋆j
, αj ∈ C if αn 6= 0, n is the degree
Table 1.2
Useful properties of ⋆-product actions on AΘ(I) elements.
Description Definition Property
“Integration” in t F˜Θ, F˜(t, s) primitive of f˜ in t, F(s, s) = 0 F˜Θ = Θ ⋆ f˜Θ
“Integration” in s F˜Θ, F˜(t, s) primitive of ˜f in s, F˜(t, t) = 0 F˜Θ = ˜fΘ ⋆ Θ
“Differentiation” in t f˜(1,0)Θ, f˜(1,0)(t, s) derivative of f˜ in t δ
′ ⋆ f˜Θ = f˜(1,0)Θ + fδ˜
“Differentiation” in s
˜f
(0,1)Θ, ˜f
(0,1)(t, s) derivative of ˜f in s
˜fΘ ⋆ δ′ = − ˜f
(0,1)Θ + ˜f δ
2. Basics and main result. In order to state the main result, we first summarize the ⋆-product basics. Refer to [16] for the general definition of this product and
the related properties. Given a bounded interval I, let us denote with A(I) the set of
the bivariate distributions of the kind
f(t, s) = ˜f−1(t, s)Θ(t − s) + ˜f0(t)δ(t − s) + ˜f1(t)δ
′
(t − s) + · · · + ˜fk(t)δ
(k)
(t − s),
where ˜f−1, . . . ,
˜fk are analytic functions over I both in t and s, Θ is the Heaviside
function (Θ(t − s) = 1 for t ≥ s, and 0 otherwise), and δ, δ′
, . . . , δ(k) are the Dirac
delta and its derivatives. Then, the ⋆-product of f1, f2 ∈ A(I) is
(f1 ⋆ f2)(t, s) := Z
I
f1(t, τ)f2(τ, s) dτ ∈ A(I).
Some of the important properties, definitions, and facts about the ⋆-product can be
found in Tables 1.1 and 1.2. Specifically, it is easy to see that δ(t−s) is the ⋆-product
identity. Moreover, since A(I) is closed under ⋆-product, we can define the ⋆-powers
of f ∈ A(I), denoted as f
⋆n with the convention f
⋆0 = δ. Therefore, for x ∈ A(I),
we can define the ⋆-polynomial of degree n as
p
⋆
(t, s) := α0δ(t − s) + α1x(t, s) + α2x(t, s)
⋆2 + · · · + αnx(t, s)
⋆n
, (2.1)
with constants α0, . . . , αn ∈ C, αn 6= 0. We call P
⋆
n the set of all such ⋆-polynomials.
We define the subset AΘ(I) ⊂ A(I) of the distributions of the form f(t, s) =
˜f(t, s)Θ(t − s), with ˜f a function analytic over I
2
. The ⋆-resolvent is defined as
R
⋆
(x) := X∞
j=0
x
⋆j
.
POLYNOMIAL APPROXIMATION IN THE ⋆-FRAMEWORK 3
Note that R⋆
(x) is well-defined (i.e., convergent) for every x ∈ AΘ [6].
When A, B are matrices with compatible sizes composed of elements from A(I),
the ⋆-product straightforwardly extends to a matrix-matrix (or matrix-vector) ⋆-
product. In the following, we denote with AN×M (I) and A
N×M
Θ (I) the spaces of
N × M matrices with elements from those sets. We denote with I⋆ = ˜Iδ(t − s) the
identity matrix in AN×N (I), with ˜I the standard N × N identity matrix.
Setting I = [a, b], the solution ˜u(t) of the ODE (1.1) can be expressed by
u˜(t) = U(t, a)˜v, t ∈ I, U(t, s) = Θ(t − s) ⋆ R⋆

A˜(t)Θ(t − s)

; (2.2)
as proven in [6]. From now on, we will skip the distribution arguments t, s whenever
it is useful and clear from the context. Since R⋆
(A˜Θ) is the ⋆-inverse of I⋆ −A˜Θ (e.g.,
[12]), then solving (2.2) means solving the system of ⋆-linear equations
(I⋆ − A˜Θ) ⋆ x = ˜vδ, u˜(t) = (Θ ⋆ x)(t, a) t ∈ I.
Note that this is not just a theoretical result since there is an efficient way to transform
the ⋆-linear system into a usual linear system that can be solved numerically [13, 14,
15].
It is reasonable to consider a ⋆-polynomial approximation p
⋆
(A˜Θ)˜v ≈ R⋆
(A˜Θ)˜v.
Specifically, we aim at finding the best ⋆-polynomial p
⋆
(t, s) of degree n that approximates the ⋆-resolvent R⋆
(A)˜v in the L2 norm sense, i.e., the polynomial q
⋆
that
minimizes the error
ku˜(t) − (Θ ⋆ q⋆
(A)˜v)(t, a)kL2
:= Z b
a
|u˜(τ) − (Θ ⋆ q⋆
(A))(τ, a)˜v|
2
!1
2
, t ∈ I.
Note that Θ ⋆ q⋆
(A) ∈ AN×N
Θ , while q
⋆
(A) ∈ AN×N .
Theorem 2.1 (Main result). Consider the initial value problem (1.1) and let
λ˜
1(t), . . . , λ˜N (t) be the eigenvalues of A˜(t) ∈ C
N×N . We define the interval
J := 
min
t∈I,i=1,...,N
λ˜
i(t), max
t∈I,i=1,...,N
λ˜
i(t)

× length(I),
and denote with En(J) the minimal uniform error of the polynomial approximation
of the exponential over J, i.e.,
En(J) := min
p∈Pn
max
t∈J
| exp(t) − p(t)|.
Define A(t, s) = A˜(t)Θ(t−s). Then the error of the L2-best ⋆-polynomial approximant
q
⋆
can be bounded by
ku˜(t) − (Θ ⋆ q⋆
(A)˜v)(t, a)kL2 ≤ En(J) ≤ M ρn+1, t ∈ I
for some constant M > 0 and 0 < ρ < 1 depending on J.
The proof of this Theorem will be the outcome of the rest of the paper. The first
step towards the proof is to derive an explicit form for the ⋆-monomials f
⋆n in the
case in which f(t, s) = ˜f(t)Θ(t − s) ∈ AΘ(I).
4 S. Pozza
Lemma 2.2. Consider the function f(t, s) = ˜f(t)Θ(t − s) ∈ AΘ and let F˜(t) be a
primitive of ˜f(t). Then, for n = 1, 2, . . .,
f(t, s)
⋆n =
˜f(t)
(n − 1)!

F˜(t) − F˜(s)
n−1
Θ(t − s), (2.3)
Θ(t − s) ⋆ f(t, s)
⋆n =
1
n!

F˜(t) − F˜(s)
n
Θ(t − s). (2.4)
Moreover, Θ(t − s) ⋆ f(t, s)
⋆0 = Θ(t − s) since f(t, s)
⋆0 = δ(t − s) by convention.
Proof. For n = 2, the expression (2.3) is trivially obtained by
f(t, s)
⋆2 = ˜f(t)Θ(t − s)
Z t
s
˜f(τ) dτ = ˜f(t)

F˜(t) − F˜(s)

Θ(t − s).
Now, by induction, assuming (2.3) we get
f(t, s)
⋆n+1 =
˜f(t)
(n − 1)!Θ(t − s)
Z t
s
˜f(τ)

F˜(τ) − F˜(s)
n−1
dτ. (2.5)
Integrating by part gives
Z t
s
˜f(τ)

F˜(τ) − F˜(s)
n−1
dτ = (F˜(t) − F˜(s))n−
(n − 1) Z t
s
˜f(τ)

F˜(τ) − F˜(s)
n−1
dτ.
Therefore,
n
Z t
s
˜f(τ)

F˜(τ) − F˜(s)
n−1
dτ = (F˜(t) − F˜(s))n
.
Together with (2.5), this proves (2.3). Eq. (2.4) comes from observing that
Θ(t − s) ⋆ f(t, s)
⋆n = Θ(t − s) ⋆
˜f(t)
(n − 1)!

F˜(t) − F˜(s)
n−1
Θ(t − s)
=
Θ(t − s)
(n − 1)! Z t
s
˜f(τ)

F˜(τ) − F˜(s)
n−1
dτ
=
1
n!
(F˜(t) − F˜(s))nΘ(t − s),
which concludes the proof.
An immediate consequence of Lemma 2.2 is that
exp 
F˜(t) − F˜(s)

= Θ(t − s) ⋆ R⋆
(f)(t, s),
a well-known result; see, e.g., [6].
3. Matrix spectral decomposition and the ⋆-product. Consider a timedependent N × N Hermitian matrix-valued function A˜(t) analytic over the closed
interval I. Then, for every t ∈ I there exist matrix-valued functions Q˜(t) and Λ( ˜ t)
analytic over I such that:
A˜(t) = Q˜(t)Λ( ˜ t)Q˜(t)
H , with Λ( ˜ t) = diag(λ˜
1(t), . . . , λ˜
n(t)), Q˜(t)
H Q˜(t) = I, (3.1)
POLYNOMIAL APPROXIMATION IN THE ⋆-FRAMEWORK 5
for every t ∈ I; see [9, Chapter II, Section 6] (we refer to [4] for extensions to the
non-analytic case). The elements of the diagonal matrix Λ( ˜ t) are analytic functions
and, for every t ∈ I, the λ˜
j (t) are the eigenvalues (eigencurves) of A˜(t). The columns
of Q˜(t), denoted ˜q1(t), . . . , q˜N (t), are the corresponding eigenvectors (analytic over I).
Given A(t, s) ∈ AN×N
Θ (I), the ⋆-eigenproblem is to find the ⋆-eigenvalues λ(t, s) ∈
AΘ(I) and the ⋆-eigenvector q(t, s) ∈ AN×1
(I) such that
A(t, s) ⋆ q(t, s) = λ(t, s) ⋆ q(t, s). (3.2)
If λ(t, s) and q(t, s) exist, then q(t, s)⋆a(t, s) is also a ⋆-eigenvector, for every a(t, s) 6≡ 0
from A(I). For the specific case of interest, where A(t, s) = A˜(t)Θ(t−s), the solution
to the ⋆-eigenproblem is in the following theorem.
Theorem 3.1. Let A(t, s) = A˜(t)Θ(t−s) be in AΘ(I), and let λ˜
i(t) and q˜i(t), be
the (analytic) eigencurves and the corresponding eigenvectors as defined in (3.1) for
i = 1, . . . , N. Then, the solution to the ⋆-eigenvalue problem (3.2) is given by
λi(t, s) = λ˜
i(t)Θ(t − s), qi(t, s) = ˜q
′
i
(t)Θ(t − s) + ˜qi(t)δ(t − s), i = 1, . . . , N.
where q˜
′
i
(t) is the derivative of q˜i(t).
Proof. First, note that
λ˜
i(t)δ(t − s) ⋆ q˜i(t)Θ(t − s) = λ˜
i(t)
Z
I
δ(t − τ)˜qi(τ)Θ(τ − s) dτ
= λ˜
i(t)˜qi(t)Θ(t − s) = A˜(t)˜qi(t)Θ(t − s)
= A˜(t)δ(t − s) ⋆ q˜i(t)Θ(t − s).
Using the fact that λ˜
i(t)δ(t−s)⋆Θ(t−s) = λ˜
i(t)Θ(t−s), and that δ
′
(t−s)⋆Θ(t−s) =
Θ(t − s) ⋆ δ′
(t − s) = δ(t − s), see Table 1.1, we obtain (we omit the variables for the
sake of readability)
λ˜
iδ ⋆ q˜iΘ = λ˜
iδ ⋆ Θ ⋆ δ′
⋆ q˜iΘ = λ˜
iΘ ⋆ δ′
⋆ q˜iΘ = λ˜
iΘ ⋆ qi
,
where qi(t, s) := δ
′
(t − s) ⋆ q˜i(t)Θ(t − s). Similarly, Aδ ⋆ ˜ q˜iΘ = A˜Θ ⋆ qi
. Combining
these results, we get
λ˜
iΘ ⋆ qi = λ˜
iδ ⋆ q˜iΘ = λ˜
iq˜iΘ = A˜q˜iΘ = Aδ ⋆ ˜ q˜iΘ = A˜Θ ⋆ qi
.
Finally, we obtain the following expression for the ⋆-eigenvectors:
qi(t, s) = δ
′
(t − s) ⋆ q˜i(t)Θ(t − s)
= ˜q
′
i
(t)Θ(t − s) + ˜qi(t)δ(t − s);
see Table 1.2. As a final remark, note that all the ⋆-products are well-defined thanks
to the fact that the λ˜
i(t) and ˜qi(t) are analytic functions.
Consider the matrix
A(t, s) = A˜−1(t, s)Θ(t − s) +X
k
j=0
A˜
j (t)δ
(j)
(t − s) ∈ AN×M (I) (3.3)
we define the Hermitian transpose of A as
A
H(t, s) = A˜H
−1
(t, s)Θ(t − s) +X
k
j=0
A˜H
j
(t)δ
(j)
(t − s) ∈ AM×N (I), (3.4)
6 S. Pozza
with A˜H
j
the usual Hermitian transpose of a matrix. As an immediate consequence
of Theorem 3.1, we have the following ⋆-factorization of A(t, s).
Corollary 3.2. Under the same assumption of Theorem 3.1, we have
A(t, s) = Q(t, s) ⋆ Λ(t, s) ⋆ Q(t, s)
H,
with Λ(t, s) = Λ( ˜ t)Θ(t − s) and Q(t, s) = [q1(t, s), . . . , qN (t, s)]. Moreover, it holds
Q(t, s) ⋆ Q(t, s)
H = Q(t, s)
H ⋆ Q(t, s) = I⋆,
that is, Q(t, s)
H is the matrix ⋆-inverse of Q(t, s).
Proof. We first show that for every i, j = 1, . . . , N we have
qi(t, s)
H ⋆ qj (t, s) = δij δ(t − s),
with δij the Kronecker delta. Since qk(t, s) = δ
′
(t − s) ⋆ q˜kΘ(t − s), for k = 1, . . . , N,
then
qi(t, s)
H ⋆ qj (t, s) =
δ
′
(t − s) ⋆ q˜
H
i
(t)Θ(t − s)

⋆

δ
′
(t − s) ⋆ q˜j (t)Θ(t − s)

= δ
′
(t − s) ⋆

q˜
H
i
(t)Θ(t − s) ⋆ δ′
(t − s)

⋆ q˜j (t)Θ(t − s)
= δ
′
(t − s) ⋆

q˜
H
i
(t)δ(t − s) ⋆ q˜j (t)Θ(t − s)

= δ
′
(t − s) ⋆ q˜
H
i
(t)˜qj (t)Θ(t − s) = δ
′
(t − s) ⋆ δijΘ(t − s)
= δij δ(t − s).
From Theorem 3.1 we get the equality
A(t, s) ⋆ Q(t, s) = Q(t, s) ⋆ Λ(t, s).
The conclusion follows from ⋆-multiplying from the right by Q(t, s)
H.
Since our final goal is to measure an error, we need to introduce a ⋆-inner product
and the relative ⋆-norm. To this aim, we take inspiration from the results in [16], but
we develop them in a different direction. Following [16], we define the ⋆-Hermitiantranspose of A(t, s) in (3.3) as
A
⋆H(t, s) := A
H(s, t) = A˜H
−1
(s, t)Θ(s − t) +X
k
j=0
A˜H
j
(s)δ
(j)
(s − t) ∈ AM×N (I).
Roughly speaking, one has to take the usual Hermitian transpose and then swap the
variable t, s. Note the difference with the Hermitian transpose (3.4). Now, setting
I = [a, b], and given v, w such that Θ ⋆ v, Θ ⋆ w ∈ AN×1
Θ (I), for any fixed s ∈ [a, b) we
can define the inner product:
hv, wi⋆(s) :=
(Θ ⋆ v)
⋆H ⋆ Θ ⋆ w
(s, s) = Z
I
v
H (τ, s)w(τ, s) dτ.
Note that, denoting Θ(t − s) ⋆ v(t, s) = V˜ (t, s)Θ(t − s) and Θ(t − s) ⋆ w(t, s) =
W˜ (t, s)Θ(t − s), then
hv, wi⋆(s) = Z b
s
V˜ H(τ, s)W˜ (τ, s
